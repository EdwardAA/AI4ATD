{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeRJFjJo48p/3ZoyKmhRxx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import csv\n","import collections\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import seaborn as sns\n","import pandas as pd"],"metadata":{"id":"RzOP6lp8RtbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_dim = 10\n","epsilon = 0.0001"],"metadata":{"id":"nGyskJzISP-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TncVNlDpQdKH"},"outputs":[],"source":["drive.mount('/content/drive')\n","%cd /write/your/directory/to/AI_4_ATD"]},{"cell_type":"markdown","source":["# Prepare Hall Data"],"metadata":{"id":"hqwnPlIdW8K2"}},{"cell_type":"markdown","source":["## Load Real Data"],"metadata":{"id":"5kIpagGPW_DD"}},{"cell_type":"code","source":["# List all CSV files in the 'Hall_Data' directory\n","file_names = os.listdir('Hall_Data')\n","# Remove a specific problematic file that was missing session labels\n","file_names.remove('5. Ahearn_Nicki.csv')\n","\n","# Define the main experimental conditions/lines for analysis\n","lines = ['control', 'tangible', 'demand', 'attention']\n","\n","# Create a lookup dictionary to normalize various header names to the standard 'lines'\n","lookup = {'play': 'control',\n","          'free play': 'control',\n","          'toy play': 'control',\n","          'att': 'attention',\n","          'attn': 'attention',\n","          'escape': 'demand',\n","          'tangibles': 'tangible',}\n","\n","# Initialize a dictionary to store processed Hall graph data for each subject\n","hall_graph_data = {}\n","# Iterate through each file found in the 'Hall_Data' directory\n","for f_name in file_names:\n","  # Process only CSV files\n","  if f_name.endswith('.csv'):\n","    # Open and read the CSV file\n","    file = open('../Hall_Data/'+f_name, newline='', encoding='latin-1')\n","    csv_reader = csv.reader(file)\n","\n","    # Initialize a graph dictionary for the current subject, with empty lists for each line\n","    graph = {}\n","    for line in lines:\n","      graph[line] = []\n","\n","    # Dictionary to map header names to column indices\n","    col = {}\n","    first_row = True\n","    # Iterate through rows in the CSV file\n","    for row in csv_reader:\n","      # Process the header row to identify column indices for each condition\n","      if first_row:\n","        for i, header in enumerate(row):\n","          if header.lower().strip() in lines:\n","            col[header.lower().strip()] = i\n","          elif header.lower().strip() in lookup:\n","            col[lookup[header.lower().strip()]] = i\n","          else:\n","            pass # Ignore irrelevant headers\n","        first_row = False\n","      # Process data rows\n","      else:\n","        # Populate graph with float values for each line/condition\n","        for line in lines:\n","          if line in col:\n","            if row[col[line]] != '': # Ensure data is not empty\n","              graph[line].append(float(row[col[line]]))\n","\n","  # Check if 'control' condition exists and at least two conditions are present\n","  if 'control' in col and len(col) >= 2:\n","    # Store the processed graph data using the subject ID as key\n","    hall_graph_data[int(f_name.split('.')[0])] = graph\n","  elif 'control' not in col:\n","    # Print a message if the 'control' condition is missing\n","    print('Missing Control/Test Condition')\n","\n","# Identify subjects to be removed if any of their data lines exceed the maximum dimension\n","del_subs = set()\n","for sub in hall_graph_data:\n","  for line in hall_graph_data[sub]:\n","    if len(hall_graph_data[sub][line]) > max_dim:\n","      del_subs.add(sub)\n","      break\n","\n","# Remove identified subjects from the dataset\n","for sub in del_subs:\n","  del hall_graph_data[sub]\n","\n","# Pad data lines with -1 if they are shorter than 'max_dim' to ensure consistent length\n","for sub in hall_graph_data:\n","  for line in hall_graph_data[sub]:\n","    cur_len = len(hall_graph_data[sub][line])\n","    hall_graph_data[sub][line] = np.append(hall_graph_data[sub][line], [-1]*(max_dim-cur_len))\n"],"metadata":{"id":"0eyX0O-MRw_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IRA"],"metadata":{"id":"95DB-e3IXWvM"}},{"cell_type":"markdown","source":["### CDC Analysis"],"metadata":{"id":"XAcH7WORXiRY"}},{"cell_type":"code","source":["CDC = {}\n","all_subs = set(hall_graph_data.keys())\n","# Iterate through each subject to perform CDC analysis\n","for sub in all_subs:\n","  if True:\n","    # Extract control data for the current subject\n","    control = hall_graph_data[sub]['control']\n","    # Filter out placeholder values (-1)\n","    control = [i for i in control if i != -1]\n","    # Calculate mean and standard deviation of the control data\n","    mean = np.mean(control)\n","    sd = np.std(control, ddof=1)\n","    # Calculate Upper Control (UC) and Lower Control (LC) limits\n","    UC = mean + sd\n","    LC = (mean - sd).clip(min=0)\n","\n","    CDC_lbl = []\n","    # Iterate through each experimental line (tangible, demand, attention)\n","    for line in hall_graph_data[sub]:\n","      above = 0\n","      below = 0\n","      if line != 'control':\n","        # Extract data for the current experimental line\n","        line_data = [i for i in hall_graph_data[sub][line] if i != -1]\n","        # Count data points above UC and below LC\n","        for point in line_data:\n","          if point != -1:\n","            if point > UC:\n","              above += 1\n","            elif point < LC:\n","              below += 1\n","        # Determine CDC label based on the proportion of points above/below control limits\n","        if len(line_data) == 0:\n","          CDC_lbl.append(0)\n","        else:\n","          diff = (above - below) / len(line_data) >= 0.5\n","          if diff:\n","            CDC_lbl.append(1)\n","          else:\n","            CDC_lbl.append(0)\n","    # Store the CDC labels for the current subject\n","    CDC[sub] = CDC_lbl"],"metadata":{"id":"xxa42jOnWwUc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Ratings"],"metadata":{"id":"zwjB4O6cX6f_"}},{"cell_type":"code","source":["def lists_identical(lists):\n","    # Compares if all lists within a list of lists are identical.\n","    # It takes the first list as a reference.\n","    first_list = lists[0]\n","    # Iterates through the rest of the lists.\n","    for lst in lists[1:]:\n","        # If any list is not identical to the first, return False.\n","        if lst != first_list:\n","            return False\n","    # If all lists are identical, return True.\n","    return True"],"metadata":{"id":"LPPjP0m2bXZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_data = {}\n","\n","h_all_subs = set()\n","h_err_subs = set()\n","\n","\n","files = os.listdir('Ratings')\n","print('loaded: ', end='')\n","# Loop through rating files to load Hall data ratings\n","for f_name in files:\n","  # Only process files starting with 'rh-'\n","  if f_name.startswith('rh-') == False:\n","    continue\n","  name = f_name[3:-4]\n","  h_data[name] = {}\n","  print(f_name, end=', ')\n","  file = open('Ratings/'+f_name)\n","  csv_reader = csv.reader(file)\n","  next(csv_reader) # Skip header row 1\n","  next(csv_reader) # Skip header row 2\n","  # Process each row to extract subject ratings\n","  for row in csv_reader:\n","    # Add subject to the set of all subjects\n","    h_all_subs.add(int(row[0]))\n","    # If there's no reported error for the subject, store their ratings\n","    if row[7] != '1':\n","      sub = int(row[0])\n","      h_data[name][sub] = [int(row[1]), int(row[2]), int(row[3])]\n","    # Otherwise, add the subject to the error set\n","    else:\n","      h_err_subs.add(int(row[0]))\n","\n","# Determine valid subjects by removing subjects with errors from all subjects\n","h_val_subs = h_all_subs - h_err_subs\n","\n","h_agr_subs = set()\n","# Identify subjects where all raters (for Hall data) completely agree\n","for sub in h_val_subs:\n","  temp = []\n","  for key in h_data:\n","    # Collect ratings from all raters for the current subject\n","    temp.append(h_data[key][sub])\n","  # Check if all collected rating lists are identical\n","  if lists_identical(temp):\n","    h_agr_subs.add(sub)\n","\n","print()\n","# Print summary statistics for Hall data subjects\n","print('# of all_subs:', len(h_all_subs)) # total subjects\n","print('# of err_subs:', len(h_err_subs)) # subjects with reported errors\n","print('# of val_subs:', len(h_val_subs)) # total subjects without reported errors\n","print('# of agr_subs:', len(h_agr_subs)) # total subjects with agreement between 3 raters"],"metadata":{"id":"VjNN4dNnYCFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove Subjects With Reported Errors"],"metadata":{"id":"77kTqjfGy42_"}},{"cell_type":"code","source":["for key in h_data:\n","  for sub in h_err_subs:\n","    try:\n","      del h_data[key][sub]\n","    except:\n","      pass"],"metadata":{"id":"mSOcbWUby2Hf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Disagreements"],"metadata":{"id":"S_i3MeSlegKi"}},{"cell_type":"code","source":["def intersection(list1, list2):\n","  # Ensure both input dictionaries have the same keys (subjects)\n","  assert list1.keys() == list2.keys()\n","  subs = list1.keys()\n","  agreement = set()\n","  # Iterate through subjects and identify where ratings are identical between the two lists\n","  for sub in list1:\n","    if list1[sub] == list2[sub]:\n","      agreement.add(sub)\n","  # Return the set of subjects with agreement\n","  return agreement"],"metadata":{"id":"tjaQvI2kfszR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["K_C = h_val_subs - intersection(h_data['katie_VA'], h_data['CDC'])\n","N_C = h_val_subs - intersection(h_data['neely_VA'], h_data['CDC'])\n","K_N = h_val_subs - intersection(h_data['katie_VA'], h_data['neely_VA'])\n","\n","# Combine all sets of disagreements to get a comprehensive list of subjects with any disagreement\n","dis = K_C | N_C | K_N\n","\n","# Open a new CSV file to write the disagreements\n","file = open('Ratings/hall_differences.csv', 'w', newline='')\n","writer = csv.writer(file)\n","# Sort the subjects with disagreements for consistent output\n","dis = sorted(dis)\n","# Write each subject's ID and their ratings from different raters to the CSV file\n","for sub in dis:\n","  writer.writerow([sub] + h_data['CDC'][sub] + h_data['katie_VA'][sub] + h_data['neely_VA'][sub])\n","file.close()"],"metadata":{"id":"Tlr8IjWleKTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Ground Truth"],"metadata":{"id":"V4V7SDtugkMg"}},{"cell_type":"code","source":["f_name = 'rh-Ground_Truth.csv'\n","name = f_name[3:-4]\n","h_data[name] = {}\n","print('loaded: ', end='')\n","print(f_name, end=', ')\n","file = open('Ground_Truth/'+f_name)\n","csv_reader = csv.reader(file)\n","next(csv_reader)\n","next(csv_reader)\n","# Process each row to extract ground truth ratings\n","for row in csv_reader:\n","  # Only include subjects without reported errors\n","  if row[7] != '1':\n","    sub = int(row[0])\n","    h_data[name][sub] = [int(row[1]), int(row[2]), int(row[3])]\n","\n","# Remove subjects with reported errors from the ground truth data if they exist\n","for sub in h_err_subs:\n","  if sub in h_data['Ground_Truth']:\n","    del h_data['Ground_Truth'][sub]"],"metadata":{"id":"A5ejyEKugxbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Friedel Data"],"metadata":{"id":"kYowZM078eXo"}},{"cell_type":"markdown","source":["## Load Real Data"],"metadata":{"id":"peYSWt908eXq"}},{"cell_type":"code","source":["# load both datasets and load them into a new combined list\n","\n","all_subs = set()\n","\n","# Load the first Friedel data file\n","file = open('Friedel_Data/fa_data_1_mod.csv', newline='', encoding='latin-1')\n","raw_data = []\n","csv_reader = csv.reader(file)\n","skip_first = True\n","for row in csv_reader:\n","  if skip_first:\n","    skip_first = False\n","    continue\n","  all_subs.add(int(row[3]))\n","  raw_data.append(row)\n","\n","# Load the second Friedel data file and append to raw_data\n","file = open('Friedel_Data/fa_data_2_mod.csv', newline='', encoding='latin-1')\n","csv_reader = csv.reader(file)\n","skip_first = True\n","for row in csv_reader:\n","  if skip_first:\n","    skip_first = False\n","    continue\n","  all_subs.add(int(row[3]))\n","  raw_data.append(row)\n","\n","# Define the lines/conditions for the graphs\n","lines = ['control', 'tangible', 'demand', 'attention']\n","\n","graph_data = {}\n","\n","# Initialize graph_data structure for each subject and line\n","for sub in all_subs:\n","  graph_data[sub] = {}\n","  for line in lines:\n","    graph_data[sub][line] = []\n","\n","# Populate graph_data with values from raw_data\n","for row in raw_data:\n","  sub = int(row[3])\n","  line = row[1].lower()\n","  val = float(row[2])\n","  if line in lines:\n","    graph_data[sub][line].append(val)\n","\n","# Identify subjects to be deleted if any line's length exceeds max_dim\n","del_subs = set()\n","for sub in all_subs:\n","  for line in graph_data[sub]:\n","    cur_len = len(graph_data[sub][line])\n","    if cur_len <= max_dim:\n","      # Pad lines with -1 if they are shorter than max_dim\n","      graph_data[sub][line] = np.append(np.array(graph_data[sub][line]), [-1]*(max_dim - cur_len))\n","    else:\n","      del_subs.add(sub)\n","      break\n","\n","# Remove subjects marked for deletion\n","for sub in del_subs:\n","  del graph_data[sub]\n","  print('removed:',sub)\n","\n","# Sort the graph_data dictionary by subject ID\n","graph_data = dict(collections.OrderedDict(sorted(graph_data.items())))"],"metadata":{"id":"fPWqFHG98eXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IRA"],"metadata":{"id":"K9sQ6pec8eXr"}},{"cell_type":"markdown","source":["### CDC Analysis"],"metadata":{"id":"rfOPlyky8eXs"}},{"cell_type":"code","source":["CDC = {}\n","all_subs = set(graph_data.keys())\n","# Iterate through each subject to perform CDC analysis\n","for sub in all_subs:\n","  if True:\n","    # Extract and clean control data for the current subject\n","    control = graph_data[sub]['control']\n","    control = [i for i in control if i != -1]\n","    # Calculate mean and standard deviation of the control data\n","    mean = np.mean(control)\n","    sd = np.std(control, ddof=1)\n","    # Calculate Upper Control (UC) and Lower Control (LC) limits\n","    UC = mean + sd\n","    LC = (mean - sd).clip(min=0)\n","\n","    CDC_lbl = []\n","    # Iterate through each experimental line (tangible, demand, attention)\n","    for line in graph_data[sub]:\n","      above = 0\n","      below = 0\n","      if line != 'control':\n","        # Extract and clean data for the current experimental line\n","        line_data = [i for i in graph_data[sub][line] if i != -1]\n","        # Count data points above UC and below LC\n","        for point in line_data:\n","          if point != -1:\n","            if point > UC:\n","              above += 1\n","            elif point < LC:\n","              below += 1\n","        # Determine CDC label (0 or 1) based on the proportion of points outside control limits\n","        if len(line_data) == 0:\n","          CDC_lbl.append(0)\n","        else:\n","          diff = (above - below) / len(line_data) >= 0.5\n","          if diff:\n","            CDC_lbl.append(1)\n","          else:\n","            CDC_lbl.append(0)\n","    # Store the determined CDC labels for the subject\n","    CDC[sub] = CDC_lbl"],"metadata":{"id":"OBl2t6CF8eXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Ratings"],"metadata":{"id":"wFaRlGNU8eXs"}},{"cell_type":"code","source":["r_data = {}\n","\n","r_all_subs = set()\n","r_err_subs = set()\n","\n","\n","files = os.listdir('Ratings')\n","print('loaded: ', end='')\n","# Loop through rating files to load Friedel data ratings\n","for f_name in files:\n","  # Only process files starting with 'r-'\n","  if f_name.startswith('r-') == False:\n","    continue\n","  name = f_name[2:-4]\n","  r_data[name] = {}\n","  print(f_name, end=', ')\n","  file = open('Ratings/'+f_name)\n","  csv_reader = csv.reader(file)\n","  next(csv_reader)\n","  next(csv_reader)\n","  # Process each row to extract subject ratings\n","  for row in csv_reader:\n","    # Add subject to the set of all subjects\n","    r_all_subs.add(int(row[1]))\n","    # If there's no reported error for the subject, store their ratings\n","    if row[8] != '1':\n","      sub = int(row[1])\n","      r_data[name][sub] = [int(row[2]), int(row[3]), int(row[4])]\n","    # Otherwise, add the subject to the error set\n","    else:\n","      r_err_subs.add(int(row[1]))\n","\n","# Determine valid subjects by removing subjects with errors from all subjects\n","r_val_subs = r_all_subs - r_err_subs\n","\n","r_agr_subs = set()\n","# Identify subjects where all raters (for Friedel data) completely agree\n","for sub in r_val_subs:\n","  temp = []\n","  for key in r_data:\n","    # Collect ratings from all raters for the current subject\n","    temp.append(r_data[key][sub])\n","  # Check if all collected rating lists are identical\n","  if lists_identical(temp):\n","    r_agr_subs.add(sub)\n","\n","print()\n","# Print summary statistics for Friedel data subjects\n","print('# of all_subs:', len(r_all_subs)) # total subjects\n","print('# of err_subs:', len(r_err_subs)) # subjects with reported errors\n","print('# of val_subs:', len(r_val_subs)) # total subjects without reported errors\n","print('# of agr_subs:', len(r_agr_subs)) # total subjects with agreement between 3 raters"],"metadata":{"id":"fkMB4ygl8eXu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Remove Subjects With Reported Errors"],"metadata":{"id":"eC2cNjho8eXu"}},{"cell_type":"code","source":["for key in r_data:\n","  for sub in r_err_subs:\n","    try:\n","      del r_data[key][sub]\n","    except:\n","      pass"],"metadata":{"id":"Vz0mLySP8eXu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Disagreements"],"metadata":{"id":"Z_3-VEzq8eXv"}},{"cell_type":"code","source":["def intersection(list1, list2):\n","  # Ensure both input dictionaries have the same keys (subjects)\n","  assert list1.keys() == list2.keys()\n","  subs = list1.keys()\n","  agreement = set()\n","  # Iterate through subjects and identify where ratings are identical between the two lists\n","  for sub in list1:\n","    if list1[sub] == list2[sub]:\n","      agreement.add(sub)\n","  return agreement"],"metadata":{"id":"yQt9CNKB8eXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# determine and save disagreements between raters\n","K_C = h_val_subs - intersection(h_data['katie_VA'], h_data['CDC'])\n","N_C = h_val_subs - intersection(h_data['neely_VA'], h_data['CDC'])\n","K_N = h_val_subs - intersection(h_data['katie_VA'], h_data['neely_VA'])\n","\n","dis = K_C | N_C | K_N\n","\n","file = open('Ratings/friedel_differences.csv', 'w', newline='')\n","writer = csv.writer(file)\n","dis = sorted(dis)\n","for sub in dis:\n","  writer.writerow([sub] + h_data['CDC'][sub] + h_data['katie_VA'][sub] + h_data['neely_VA'][sub])\n","file.close()"],"metadata":{"id":"lHDaETEz8eXw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Ground Truth"],"metadata":{"id":"QWh8-40t8eXw"}},{"cell_type":"code","source":["f_name = 'r-Ground_Truth.csv'\n","name = f_name[2:-4]\n","r_data[name] = {}\n","print('loaded: ', end='')\n","print(f_name, end=', ')\n","file = open('Ground_Truth/'+f_name)\n","csv_reader = csv.reader(file)\n","next(csv_reader)\n","next(csv_reader)\n","# Process each row to extract ground truth ratings for Friedel data\n","for row in csv_reader:\n","  # Only include subjects without reported errors\n","  if row[8] != '1':\n","    sub = int(row[1])\n","    r_data[name][sub] = [int(row[2]), int(row[3]), int(row[4])]\n","\n","# Remove subjects with reported errors from the ground truth data if they exist\n","for sub in r_err_subs:\n","  if sub in r_data['Ground_Truth']:\n","    del r_data['Ground_Truth'][sub]"],"metadata":{"id":"rpsiE7r38eXw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Merge Datasets and GT Labels\n","\n"],"metadata":{"id":"9dlD8eioSaj2"}},{"cell_type":"code","source":["r_val_subs = sorted(r_val_subs)\n","h_val_subs = sorted(h_val_subs)\n","\n","subs = []\n","# Create a list of all valid subjects, distinguishing between Friedel ('r') and Hall ('h') data\n","for sub in r_val_subs:\n","  subs.append([sub, 'r'])\n","for sub in h_val_subs:\n","  subs.append([sub, 'h'])\n","\n","# Create a lookup dictionary to map new sequential subject IDs to original subject IDs and dataset types\n","sub_lookup = {}\n","for i, sub in enumerate(subs):\n","  sub_lookup[i] = sub\n","\n","combined_graph_data = {}\n","# Populate combined_graph_data by taking graphs from either Friedel or Hall data based on the lookup\n","for new_sub in sub_lookup:\n","  sub = sub_lookup[new_sub]\n","  if sub[1] == 'r':\n","    combined_graph_data[new_sub] = graph_data[sub[0]]\n","  elif sub[1] == 'h':\n","    combined_graph_data[new_sub] = hall_graph_data[sub[0]]\n","  else:\n","    print('error')\n","\n","combined_ratings = {}\n","# Populate combined_ratings for each rater (key) by merging ratings from Friedel and Hall data\n","for key in r_data:\n","  combined_ratings[key] = {}\n","  for new_sub in sub_lookup:\n","    sub = sub_lookup[new_sub]\n","    if sub[1] == 'r':\n","      combined_ratings[key][new_sub] = r_data[key][sub[0]]\n","    elif sub[1] == 'h':\n","      combined_ratings[key][new_sub] = h_data[key][sub[0]]\n","    else:\n","      print('error')\n","\n","# Extract combined ground truth labels from the combined_ratings\n","combined_labels = combined_ratings['Ground_Truth']"],"metadata":{"id":"np7W5Sx4XPmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = []\n","# Structure the combined graph data into a list of lists, where each inner list represents a graph\n","for sub in combined_graph_data:\n","  graph = []\n","  for line in combined_graph_data[sub]:\n","    graph.append(combined_graph_data[sub][line])\n","  dataset.append(graph)\n","# Extract labels from the combined_labels dictionary\n","labels = [lbl for lbl in combined_labels.values()]\n","\n","# Convert the dataset and labels to numpy arrays\n","dataset = np.array(dataset)\n","labels = np.array(labels)\n","\n","# Save the processed dataset and labels to .npy files\n","np.save('Datasets/real_dataset.npy', dataset)\n","np.save('Datasets/real_labels.npy', labels)"],"metadata":{"id":"0P6eJAlReEul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Determine Effect Sizes\n","\n","Effect sizes for each condition are calculated. If the control of the graph has zero standard deviation, conditions are marked with '-2'. If a condition is a placeholder (i.e. just there to fill the 3x10 array), it is marked with a -1. If a graph is both a '-1' and '-2', it is marked as '-1'."],"metadata":{"id":"VapZIJtrdr8O"}},{"cell_type":"code","source":["def calc_effect_sizes(dataset):\n","\n","  real_effect_sizes = []\n","\n","  # Iterate through each graph in the dataset\n","  for graph in dataset:\n","    effect_sizes = []\n","    # Extract control data and filter out placeholder values (-1)\n","    control = [i for i in graph[0] if i != -1]\n","    assert len(control) != 0\n","    # Calculate mean and standard deviation of the control data\n","    c_mean = np.mean(control)\n","    c_std = np.std(control)\n","\n","    # Iterate through each experimental line (excluding control)\n","    for line in graph[1:]:\n","      # Filter out placeholder values (-1)\n","      line = [i for i in line if i != -1]\n","      # Handle cases where the line is empty (no data)\n","      if len(line) == 0:\n","        effect_sizes.append(-1)\n","      # Handle cases where control standard deviation is zero\n","      elif c_std == 0:\n","        effect_sizes.append(-2)\n","      # Calculate Cohen's d effect size for the current line\n","      else:\n","        es = np.absolute((np.mean(line) - c_mean) / c_std)\n","        effect_sizes.append(es)\n","    # Assert that no NaN values were produced in effect sizes\n","    assert not np.any(np.isnan(effect_sizes))\n","    real_effect_sizes.append(effect_sizes)\n","\n","  return real_effect_sizes"],"metadata":{"id":"eUE2VgUgdo3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_effect_sizes = calc_effect_sizes(dataset)\n","np.save('Datasets/real_effect_sizes.npy', real_effect_sizes)"],"metadata":{"id":"b65JukE7hPJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Interrater Agreement Matrices\n"],"metadata":{"id":"0hXRmkSsuwZf"}},{"cell_type":"code","source":["n = len(combined_ratings.keys())\n","agr_mat = [[0 for i in range(n)] for j in range(n)]\n","# Calculate the agreement percentage between each pair of raters/methods\n","for i, x_key in enumerate(combined_ratings):\n","  for j, y_key in enumerate(combined_ratings):\n","    agreement = intersection(combined_ratings[x_key], combined_ratings[y_key])\n","    agr_mat[i][j] = len(agreement) / len(combined_ratings[x_key])\n","\n","# Convert the error matrix to a Pandas DataFrame for easier visualization\n","agr_df = pd.DataFrame(agr_mat, columns=combined_ratings.keys(), index=combined_ratings.keys())\n","\n","# Create a mask for the upper triangle to avoid redundant information in the heatmap\n","mask = np.triu(np.ones_like(agr_df, dtype=bool))\n","\n","# Create a heatmap using Seaborn to visualize interrater agreement\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(agr_df, annot=True, cmap='rocket', fmt=\".2f\", mask=mask)\n","plt.title('Inter Rater Agreement Matrix (n='+str(len(combined_ratings[x_key]))+')\\n(Complete Agreement on Graph)')\n","plt.xlabel('Rater and Method')\n","plt.ylabel('Rater and Method')\n","plt.show()"],"metadata":{"id":"8Rn6uOEhuztZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_condition = []\n","# Iterate through each graph in the dataset to check for missing conditions\n","for graph in dataset:\n","  temp = []\n","  # Check each experimental line (excluding control) for data presence\n","  for line in graph[1:]:\n","    # If the line contains only placeholder values (-1), mark as True (missing data)\n","    if len([i for i in line if i != -1]) == 0:\n","      temp.append(True)\n","    else:\n","      temp.append(False)\n","  missing_condition.append(temp)\n","\n","# Convert the list of booleans to a numpy array and invert the values\n","# This results in True where data is present and False where it's missing.\n","missing_condition = np.logical_not(np.array(missing_condition))"],"metadata":{"id":"F_zQHS-j1KDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flattened_ratings = {}\n","for key in combined_ratings:\n","  flattened_ratings[key] = []\n","  # Flatten the combined ratings for each rater into a single list\n","  for sub in combined_ratings[key]:\n","    flattened_ratings[key].append(combined_ratings[key][sub])\n","  flattened_ratings[key] = np.array(flattened_ratings[key])\n","\n","n = len(flattened_ratings.keys())\n","agr_mat = [[0 for i in range(n)] for j in range(n)]\n","for i, x_key in enumerate(flattened_ratings):\n","  for j, y_key in enumerate(flattened_ratings):\n","    # Calculate the mean agreement on individual conditions, considering only existing data points\n","    agreement = np.mean(np.array(flattened_ratings[x_key][missing_condition]) == np.array(flattened_ratings[y_key][missing_condition]))\n","    agr_mat[i][j] = agreement\n","\n","# Convert the error matrix to a Pandas DataFrame for easier visualization\n","agr_df = pd.DataFrame(agr_mat, columns=flattened_ratings.keys(), index=flattened_ratings.keys())\n","\n","# Create a mask for the upper triangle to avoid redundant information in the heatmap\n","mask = np.triu(np.ones_like(agr_df, dtype=bool))\n","\n","# Create a heatmap using Seaborn to visualize interrater agreement on individual conditions\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(agr_df, annot=True, cmap='rocket', fmt=\".2f\", mask=mask)\n","plt.title('Inter Rater Agreement Matrix (n='+str(len(combined_ratings[x_key]))+')\\n(Agreement on Individual Conditions)')\n","plt.xlabel('Rater and Method')\n","plt.ylabel('Rater and Method')\n","plt.show()"],"metadata":{"id":"4HQMr9vrziS7"},"execution_count":null,"outputs":[]}]}