{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EZAYA08OGCC5"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import EarlyStopping\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from matplotlib.backends.backend_pdf import PdfPages\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import os\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7LayejGGJus"},"outputs":[],"source":["drive.mount('/content/drive')\n","%cd /write/your/directory/to/AI_4_ATD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jc4J_xhgGMZu"},"outputs":[],"source":["epsilon = 0.001"]},{"cell_type":"markdown","metadata":{"id":"8KabZrU__dRV"},"source":["#Define Processing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hO3xZ3Po_gx9"},"outputs":[],"source":["class Format():\n","  def __init__(self):\n","    self.scale = 0\n","    self.cns = 0\n","    self.ia = 0\n","    self.imp = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3_Q3CJ7_jZa"},"outputs":[],"source":["def SCALE(dataset, labels, format):\n","  shape = dataset.shape\n","  if len(shape) == 3:\n","    batch, n_lines, n_points = shape\n","    for i in range(batch):\n","      max_val = np.max(np.absolute([y for y in dataset[i][0]]))\n","      for j in range(n_lines - 1):\n","        new_max_val = np.max(np.absolute([y for y in dataset[i][j + 1]]))\n","        if new_max_val > max_val:\n","          max_val = new_max_val\n","      for j in range(n_lines):\n","        for k in range(n_points):\n","          if dataset[i][j][k] != -1:\n","            dataset[i][j][k] /= max_val\n","  else:\n","    batch, n_points = shape\n","    for i in range(batch):\n","      max_val = np.max(np.absolute([y for y in dataset[i]]))\n","      for j in range(n_points):\n","        if dataset[i][j] != -1:\n","          dataset[i][j] /= max_val\n","  format.scale = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rx062tm0_ksj"},"outputs":[],"source":["def CNS(dataset, labels, format):\n","  batch, n_lines, n_points = dataset.shape\n","  for i in range(batch):\n","    c_mean = np.mean([y for y in dataset[i][0] if y != -1])\n","    c_std = np.std([y for y in dataset[i][0] if y != -1]).clip(epsilon, None)\n","    for j in range(n_lines):\n","      for k in range(n_points):\n","        if dataset[i][j][k] != -1:\n","          dataset[i][j][k] = (dataset[i][j][k] - c_mean) / c_std\n","  dataset = np.delete(dataset, 0, axis=1)\n","\n","  format.cns = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gs35BXdO_mZF"},"outputs":[],"source":["def IA(dataset, labels, format):\n","\n","  if format.cns == 0:\n","    control = dataset[:, 0, :].copy()\n","    dataset = np.delete(dataset, 0, axis=1)\n","\n","  batch, n_lines, n_points = dataset.shape\n","  dataset = (dataset.copy()).reshape(batch*n_lines, n_points)\n","\n","  if format.cns == 0:\n","    control = np.repeat(control, repeats=3, axis=0)\n","    dataset = np.stack((control, dataset), axis=1)\n","    print(dataset.shape)\n","\n","  labels = labels.reshape(batch*n_lines)\n","\n","  format.ia = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJOoQVJR_plh"},"outputs":[],"source":["def iqm(series, q):\n","  q1 = np.quantile(series, 1-q)\n","  q3 = np.quantile(series, q)\n","  series = [y for y in series if q1 <= y <= q3]\n","  return np.mean(series)\n","\n","def imputate(dataset, labels, format, imp_type):\n","  shape = dataset.shape\n","  if len(shape) == 3:\n","    batch, n_series, n_points = shape\n","    for i in range(batch):\n","      for j in range(n_series):\n","        series = [y for y in dataset[i][j] if y != -1]\n","        if len(series) == 0:\n","          dataset[i][j] = np.zeros(n_points)\n","        elif imp_type == 'mean':\n","          imput_val = np.mean(series)\n","        elif imp_type == 'median':\n","          imput_val = np.median(series)\n","        elif imp_type == 'iqm':\n","          imput_val = iqm(series, 0.75)\n","        else:\n","          assert False, f\"imp_type={imp_type}\"\n","\n","        for k in range(n_points):\n","          if dataset[i][j][k] == -1:\n","            dataset[i][j][k] = imput_val\n","  else:\n","    batch, n_points = shape\n","    for i in range(batch):\n","      series = [y for y in dataset[i] if y != -1]\n","      if len(series) == 0:\n","        dataset[i] = np.zeros(n_points)\n","      elif imp_type == 'mean':\n","        imput_val = np.mean(series)\n","      elif imp_type == 'median':\n","        imput_val = np.median(series)\n","      elif imp_type == 'iqm':\n","        imput_val = iqm(series, 0.75)\n","      else:\n","        assert False, f\"imp_type={imp_type}\"\n","\n","      for j in range(n_points):\n","          if dataset[i][j] == -1:\n","            dataset[i][j] = imput_val\n","  if imp_type == 'mean':\n","    format.imp = 1\n","  elif imp_type == 'median':\n","    format.imp = 2\n","  elif imp_type == 'iqm':\n","    format.imp = 3\n","  else:\n","    assert False, f\"imp_type={imp_type}\"\n","\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vi9Mg2-B_7Kg"},"outputs":[],"source":["def process(dataset, labels, goal_format):\n","  format = Format()\n","  if goal_format.cns == 1:\n","    dataset, labels, format = CNS(dataset, labels, format)\n","  if goal_format.ia == 1:\n","    dataset, labels, format = IA(dataset, labels, format)\n","  if goal_format.imp == 1:\n","    dataset, labels, format = imputate(dataset, labels, format, 'mean')\n","  elif goal_format.imp == 2:\n","    dataset, labels, format = imputate(dataset, labels, format, 'median')\n","  elif goal_format.imp == 3:\n","    dataset, labels, format = imputate(dataset, labels, format, 'iqm')\n","  if goal_format.scale == 1:\n","    dataset, labels, format = SCALE(dataset, labels, format)\n","\n","  assert format.cns == goal_format.cns\n","  assert format.scale == goal_format.scale\n","  assert format.ia == goal_format.ia\n","  assert format.imp == goal_format.imp\n","\n","  return dataset, labels, format"]},{"cell_type":"markdown","metadata":{"id":"t3dA9wWK_RpJ"},"source":["# Run through combinations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HibM2IqMmapo"},"outputs":[],"source":["run_num = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2o6lFw_jtVh"},"outputs":[],"source":["def build_model(format):\n","  # Determine the output size and input shape based on the format's 'ia' and 'cns' flags\n","  if format.ia == 0:\n","    out_size = 3\n","    if format.cns == 0:\n","      input_shape = (4, 10)\n","    else:\n","      input_shape = (3, 10)\n","  else:\n","    out_size = 1\n","    if format.cns == 0:\n","      input_shape = (2, 10)\n","    else:\n","      input_shape = (10,)\n","\n","  # Initialize a Sequential Keras model\n","  model = tf.keras.Sequential()\n","  # Flatten the input data to a 1D vector\n","  model.add(layers.Flatten(input_shape=input_shape))\n","  # Add dense layers with ReLU activation for feature extraction\n","  model.add(layers.Dense(16, activation='relu'))\n","  model.add(layers.Dense(16, activation='relu'))\n","  # Add a Dropout layer to prevent overfitting\n","  model.add(layers.Dropout(0.2))\n","  # Add the output dense layer with softmax activation for classification\n","  model.add(layers.Dense(2*out_size, activation='softmax'))\n","  # Reshape the output if the original output size was 3, then apply softmax again\n","  if out_size == 3:\n","    model.add(layers.Reshape((3, 2)))\n","    model.add(layers.Softmax(axis=2))\n","  # Otherwise, apply softmax directly to the output\n","  else:\n","    model.add(layers.Softmax())\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dbhjqb8x9Ywz"},"outputs":[],"source":["dataset = np.load('Datasets/train_dataset.npy')\n","labels = np.load('Datasets/train_labels.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgYJRwyNDWek"},"outputs":[],"source":["seed = 42 + 7 * run_num\n","# Set random seeds for reproducibility across numpy, tensorflow, and random modules\n","random.seed(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","# Define the path for saving training history plots as a PDF\n","history_file = f'Training_Runs/SYS_Models_Run{run_num}/history.pdf'\n","# Initialize a PdfPages object to save multiple plots into a single PDF file\n","p = PdfPages(history_file)\n","\n","# List all existing model files in the current run directory\n","model_files = os.listdir(f'Training_Runs/SYS_Models_Run{run_num}')\n","# Extract format identifiers from already trained models to skip retraining\n","trained_models = [f[8:12] for f in model_files if 'FAM-SYS-' in f]\n","\n","# Configure EarlyStopping callback to prevent overfitting during training\n","early_stopping = EarlyStopping(\n","    monitor='val_accuracy',  # Monitor validation accuracy\n","    patience=15,          # Stop training if no improvement after 15 epochs\n","    restore_best_weights=True  # Restore the best model weights found during training\n",")\n","\n","# Iterate through all combinations of data processing formats (scale, cns, ia, imp)\n","for scale in range(2):\n","  for cns in range(2):\n","    for ia in range(2):\n","      for imp in range(4):\n","        # Skip training if a model with the current format combination already exists\n","        if f'{scale}{cns}{ia}{imp}' in trained_models:\n","          continue\n","\n","        # Print the current format combination being processed\n","        print(scale, cns, ia, imp)\n","        # Create a Format object to define the desired data processing steps\n","        goal_format = Format()\n","        goal_format.scale = scale\n","        goal_format.cns = cns\n","        goal_format.ia = ia\n","        goal_format.imp = imp\n","\n","        # Process the dataset and labels according to the goal_format\n","        dataset_it, labels_it, format = process(dataset.copy(), labels, goal_format)\n","\n","        # Split the processed dataset into training and validation sets\n","        X_train, X_val, y_train, y_val = train_test_split(dataset_it, labels_it, test_size=0.2, random_state=42)\n","\n","        # Build the Keras model based on the applied data format\n","        model = build_model(format)\n","\n","        # Compile the model with an optimizer, loss function, and metrics\n","        model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","        # Train the model with early stopping\n","        history = model.fit(X_train, y_train,\n","                            epochs=500,\n","                            validation_data=(X_val, y_val,),\n","                            verbose=0,\n","                            callbacks=[early_stopping])\n","\n","        # Construct a descriptive string for the model name based on applied formats\n","        format_string = ''\n","        if format.scale == 1:\n","          format_string += '_scale'\n","        if format.cns == 1:\n","          format_string += '_cns'\n","        if format.ia == 1:\n","          format_string += '_ia'\n","        if format.imp >= 1:\n","          format_string += '_imp'\n","          if format.imp == 1:\n","            format_string += '-mean'\n","          elif format.imp == 2:\n","            format_string += '-median'\n","          elif format.imp == 3:\n","            format_string += '-iqm'\n","        # Generate the full model name including format, epochs, and a unique identifier\n","        model_name = f'FAM-SYS-{format.scale}{format.cns}{format.ia}{format.imp}'+f'-ep={len(history.epoch)}'+format_string+'.keras'\n","        # Print the name of the saved model\n","        print(model_name)\n","        # Save the trained model to the specified directory\n","        model.save(f'Training_Runs/SYS_Models_Run{run_num}/'+model_name)\n","\n","        # Plot training and validation accuracy over epochs\n","        plt.plot(history.history['accuracy'], label='accuracy')\n","        plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.ylim([0.5, 1])\n","        plt.legend(loc='lower right')\n","        p.savefig() # Save the accuracy plot to the PDF\n","        plt.close() # Close the plot to free up memory\n","\n","        # Plot training and validation loss over epochs\n","        plt.plot(history.history['loss'], label='loss')\n","        plt.plot(history.history['val_loss'], label = 'val_loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.ylim([0, 1])\n","        plt.legend(loc='lower right')\n","        p.savefig() # Save the loss plot to the PDF\n","        plt.close() # Close the plot to free up memory\n","\n","# Close the PDF file after all plots have been saved\n","p.close()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjMARt5Ga+5mUnu5kaVYll"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}