{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMd+q6QYU6f/olRfI3L3beS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"r07b3Z_k6mxk"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from math import tan, atan, radians, degrees\n","import random\n","from google.colab import drive\n","import pandas as pd"]},{"cell_type":"code","source":["max_dim = 10\n","epsilon = 0.0001"],"metadata":{"id":"JV6uLd6e6yhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","%cd /write/your/directory/to/AI_4_ATD"],"metadata":{"id":"1xnJNm1g8Zgs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxUDZo5Db1nL"},"source":["# Define Processing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dSbaf2ab1nN"},"outputs":[],"source":["class Format():\n","  def __init__(self):\n","    self.scale = 0\n","    self.cns = 0\n","    self.ia = 0\n","    self.imp = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsQHkURvb1nN"},"outputs":[],"source":["def SCALE(dataset, labels, format):\n","  # Determine the shape of the dataset to handle 2D or 3D inputs\n","  shape = dataset.shape\n","  if len(shape) == 3:\n","    # Process 3D datasets (batch, n_lines, n_points)\n","    batch, n_lines, n_points = shape\n","    for i in range(batch):\n","      # Find the maximum absolute value across all lines in the current batch for scaling\n","      max_val = np.max(np.absolute([y for y in dataset[i][0] if y != -1]), initial=0)\n","      for j in range(n_lines - 1):\n","        new_max_val = np.max(np.absolute([y for y in dataset[i][j + 1] if y != -1]), initial=0)\n","        if new_max_val > max_val:\n","          max_val = new_max_val\n","      # Prevent division by zero if all values are zero\n","      if max_val == 0:\n","        max_val = epsilon\n","      # Scale all non-missing values in the current batch by the determined max_val\n","      for j in range(n_lines):\n","        for k in range(n_points):\n","          if dataset[i][j][k] != -1:\n","            dataset[i][j][k] /= max_val\n","  else:\n","    # Process 2D datasets (batch, n_points)\n","    batch, n_points = shape\n","    for i in range(batch):\n","      # Find the maximum absolute value in the current batch for scaling\n","      max_val = np.max(np.absolute([y for y in dataset[i] if y != -1]), initial=0)\n","      # Prevent division by zero if all values are zero\n","      if max_val == 0:\n","        max_val = epsilon\n","      # Scale all non-missing values in the current batch by the determined max_val\n","      for j in range(n_points):\n","        if dataset[i][j] != -1:\n","          dataset[i][j] /= max_val\n","  # Mark that scaling has been applied\n","  format.scale = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czu1jNnFb1nN"},"outputs":[],"source":["def CNS(dataset, labels, format):\n","  batch, n_lines, n_points = dataset.shape\n","  for i in range(batch):\n","    # Calculate the mean and standard deviation of the control series (first line) for the current batch\n","    c_mean = np.mean([y for y in dataset[i][0] if y != -1])\n","    c_std = np.std([y for y in dataset[i][0] if y != -1]).clip(epsilon, None)\n","    for j in range(n_lines):\n","      for k in range(n_points):\n","        # Apply Common Negative Scaling transformation to all non-missing data points using the control series' statistics\n","        if dataset[i][j][k] != -1:\n","          dataset[i][j][k] = (dataset[i][j][k] - c_mean) / c_std\n","  # Remove the control series from the dataset after scaling\n","  dataset = np.delete(dataset, 0, axis=1)\n","\n","  format.cns = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYy223Ujb1nO"},"outputs":[],"source":["def IA(dataset, labels, format):\n","\n","  # If Common Negative Scaling (CNS) was not applied, separate the control series\n","  if format.cns == 0:\n","    control = dataset[:, 0, :].copy()\n","    dataset = np.delete(dataset, 0, axis=1)\n","\n","  batch, n_lines, n_points = dataset.shape\n","  # Reshape the dataset from 3D to 2D for individual analysis of each line\n","  dataset = (dataset.copy()).reshape(batch*n_lines, n_points)\n","\n","  # If CNS was not applied, re-add the control series to each reshaped line\n","  if format.cns == 0:\n","    control = np.repeat(control, repeats=3, axis=0)\n","    dataset = np.stack((control, dataset), axis=1)\n","\n","  # Reshape labels to match the new dataset shape\n","  labels = labels.reshape(batch*n_lines)\n","\n","  # Mark that Individual Analysis (IA) has been applied\n","  format.ia = 1\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrmBxQq8b1nO"},"outputs":[],"source":["def iqm(series, q):\n","  # Calculate the interquartile mean of a series, handling empty series by returning the mean of the original series.\n","  old_series = series.copy()\n","  q1 = np.quantile(series, 1-q)\n","  q3 = np.quantile(series, q)\n","  series = [y for y in series if q1 <= y <= q3]\n","  if len(series) == 0:\n","    return np.mean(old_series)\n","  return np.mean(series)\n","\n","def imputate(dataset, labels, format, imp_type):\n","  # Imputate missing values (-1) in the dataset using the specified imputation type (mean, median, or interquartile mean).\n","  shape = dataset.shape\n","  if len(shape) == 3:\n","    batch, n_series, n_points = shape\n","    for i in range(batch):\n","      for j in range(n_series):\n","        series = [y for y in dataset[i][j] if y != -1]\n","        if len(series) == 0:\n","          dataset[i][j] = np.zeros(n_points)\n","        elif imp_type == 'mean':\n","          imput_val = np.mean(series)\n","        elif imp_type == 'median':\n","          imput_val = np.median(series)\n","        elif imp_type == 'iqm':\n","          imput_val = iqm(series, 0.75)\n","        else:\n","          assert False, f\"imp_type={imp_type}\"\n","\n","        for k in range(n_points):\n","          if dataset[i][j][k] == -1:\n","            dataset[i][j][k] = imput_val\n","  else:\n","    batch, n_points = shape\n","    for i in range(batch):\n","      series = [y for y in dataset[i] if y != -1]\n","      if len(series) == 0:\n","        dataset[i] = np.zeros(n_points)\n","      elif imp_type == 'mean':\n","        imput_val = np.mean(series)\n","      elif imp_type == 'median':\n","        imput_val = np.median(series)\n","      elif imp_type == 'iqm':\n","        imput_val = iqm(series, 0.75)\n","      else:\n","        assert False, f\"imp_type={imp_type}\"\n","\n","      for j in range(n_points):\n","          if dataset[i][j] == -1:\n","            dataset[i][j] = imput_val\n","  if imp_type == 'mean':\n","    format.imp = 1\n","  elif imp_type == 'median':\n","    format.imp = 2\n","  elif imp_type == 'iqm':\n","    format.imp = 3\n","  else:\n","    assert False, f\"imp_type={imp_type}\"\n","\n","  return dataset, labels, format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nw8tEkMb1nO"},"outputs":[],"source":["def process(dataset, labels, goal_format):\n","  # Initialize a Format object to track applied transformations\n","  format = Format()\n","  # Apply Common Negative Scaling (CNS) if specified in goal_format\n","  if goal_format.cns == 1:\n","    dataset, labels, format = CNS(dataset, labels, format)\n","  # Apply Individual Analysis (IA) if specified in goal_format\n","  if goal_format.ia == 1:\n","    dataset, labels, format = IA(dataset, labels, format)\n","  # Apply imputation (mean, median, or IQM) if specified in goal_format\n","  if goal_format.imp == 1:\n","    dataset, labels, format = imputate(dataset, labels, format, 'mean')\n","  elif goal_format.imp == 2:\n","    dataset, labels, format = imputate(dataset, labels, format, 'median')\n","  elif goal_format.imp == 3:\n","    dataset, labels, format = imputate(dataset, labels, format, 'iqm')\n","  # Apply scaling if specified in goal_format\n","  if goal_format.scale == 1:\n","    dataset, labels, format = SCALE(dataset, labels, format)\n","\n","  # Assert that all specified format changes have been applied\n","  assert format.cns == goal_format.cns\n","  assert format.scale == goal_format.scale\n","  assert format.ia == goal_format.ia\n","  assert format.imp == goal_format.imp\n","\n","  # Assert that no NaN values are present in the processed dataset\n","  assert not np.any(np.isnan(dataset))\n","\n","  return dataset, labels, format"]},{"cell_type":"markdown","source":["# Generate Training Dataset"],"metadata":{"id":"RbhXAMeh8QHi"}},{"cell_type":"code","source":["def create_series(a, t, constant, n, SMD):\n","\n","    #Start with empty series\n","    data_series = []\n","\n","    #For number of points generate errors\n","    for i in range(n):\n","\n","        #To deal with first point only\n","        if not data_series:\n","            error = np.random.normal()\n","            data_series.append(error)\n","\n","        #Points other than first - Add autocorrelation\n","        else:\n","            error = a*(data_series[i-1])+np.random.normal()\n","            data_series.append(error)\n","\n","    #Add trend to the series based on the given angle 't'\n","    middle_point = np.median(range(n))\n","\n","    for i in range(n):\n","        diff = i - middle_point\n","        data_series[i] = data_series[i] + diff*tan(radians(t))\n","\n","    #Add a constant value to each point in the series\n","    data_series = [x + constant for x in data_series]\n","\n","    #Add Standardized Mean Difference (SMD) to each point of the series\n","    for j in range(n):\n","        data_series[j] = data_series[j] + SMD\n","\n","    return data_series"],"metadata":{"id":"BVgcC7C8uaTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set random seed for replicability\n","np.random.seed(48151)\n","\n","#Set values of autocorrelation 'a', trend in degrees 't', number of points in\n","#phase A and phase B, and standardized mean difference 'smd'\n","a_values = [0,0.2]\n","t_values = [0, 0, 0]\n","constant_values = [0.5]\n","points_values = (3, 10)\n","diff_smd = [0, 3, 5, 7, 9, 10]\n","#smd_values = len(diff_smd)*[0] + diff_smd\n","smd_values = diff_smd\n","\n","series_dataset = []\n","series_labels = []\n","dataset = []\n","labels = []\n","# Generate individual time series based on a combination of parameters\n","for i in range(100):\n","    for a in a_values:\n","        for t in t_values:\n","            for constant in constant_values:\n","                for points in range(points_values[0], points_values[1] + 1):\n","                      for smd in smd_values:\n","                          dataseries = create_series(a, t, constant,\n","                                                  points, smd)\n","                          series_dataset.append(dataseries)\n","                          series_labels.append(np.clip(smd, 0, 1))\n","\n","# Pair series with their labels and shuffle them randomly\n","paired = list(zip(series_dataset, series_labels))\n","random.shuffle(paired)\n","series_dataset, series_labels = zip(*paired)\n","\n","# Create 'graphs' by combining a control series with three generated series\n","for i in range(round(len(series_dataset)/3)):\n","  # Randomly determine autocorrelation for the control series\n","  a = a_values[-1] * random.randint(0, 1)\n","  constant = constant_values[0]\n","  points = random.randint(points_values[0], points_values[1])\n","  # Generate a control series with no SMD (effect)\n","  control = create_series(a, 0, constant, points, 0)\n","  # Assemble the graph: control + three experimental series\n","  graph = [control, series_dataset[i], series_dataset[i+1], series_dataset[i+2]]\n","  # Get labels for the experimental series\n","  graph_labels = [series_labels[i], series_labels[i+1], series_labels[i+2]]\n","  dataset.append(graph)\n","  labels.append(graph_labels)\n","\n","# Pad shorter series with -1 to a maximum length (max_dim)\n","for graph in range(len(dataset)):\n","  for line in range(len(dataset[graph])):\n","    length = len(dataset[graph][line])\n","    if length < 10:\n","      for i in range(10-length):\n","        dataset[graph][line].append(-1)\n","\n","# Convert the dataset and labels to NumPy arrays\n","dataset = np.array(dataset)\n","labels = np.array(labels)\n","\n","# Define the desired processing format for the training data\n","format = Format()\n","format.scale = 1\n","format.cns = 0\n","format.ia = 0\n","format.imp = 0\n","\n","# Process the dataset according to the specified format\n","dataset, labels, format = process(dataset, labels, format)"],"metadata":{"id":"2NHFTD5JvjtB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'length of series dataset: {len(series_dataset)}')"],"metadata":{"id":"MYtyW9nZ-AE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'length of training dataset: {len(dataset)}')"],"metadata":{"id":"57SWDnfyG0T5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('Datasets/train_dataset', dataset)\n","np.save('Datasets/train_labels.npy', labels)"],"metadata":{"id":"ZIff0DoJ8exX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot Training Graph Examples"],"metadata":{"id":"CgKTr_bZZfGD"}},{"cell_type":"code","source":["def plot_ATD(ax, graph, label, idx):\n","  random.seed(346)\n","  markers = ['o', 'v', 's', 'x']\n","  legend = ['Control', 'Condition 1', 'Condition 2', 'Condition 3']\n","  # Iterate through each line in the graph to plot it\n","  for i, line in enumerate(graph):\n","    # Filter out missing values (-1) before plotting\n","    series = [y for y in line if y != -1]\n","    if len(series) > 0:\n","      # Plot the series with a specific marker and add to legend\n","      ax.plot([y for y in line if y != -1], color='k', marker = markers[i], label=legend[i])\n","  # Display the legend for the plot\n","  ax.legend()\n","  # Set the title with the graph's label\n","  ax.set_title(f\"Label={label}\")\n","  # Set axis labels\n","  ax.set_xlabel('Session')\n","  ax.set_ylabel('Behavior Response')\n","\n","# Generate three random indices to select graphs for plotting\n","random_indices = random.sample(range(len(dataset)), 3)\n","\n","# Create a figure with three subplots to display the selected graphs\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","# Plot each selected graph in its respective subplot\n","for i, idx in enumerate(random_indices):\n","  plot_ATD(axes[i], dataset[idx], labels[idx], idx)\n","\n","# Adjust layout to prevent overlapping titles/labels and display the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"wbK-BB6Aclcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Describe the Training Dataset"],"metadata":{"id":"ka4cV26nBf7o"}},{"cell_type":"markdown","source":["### Describe Dataset"],"metadata":{"id":"Ha31NoHKB33z"}},{"cell_type":"code","source":["data = {\n","    \"Descriptor\": [\"Level for Diff\", \"Level for Undiff\", \"Trend (deg)\", \"Trend (SMD/Sesh)\", \"Variability\", \"CV\"],\n","    10: [0, 0, 0, 0, 0, 0],\n","    50: [0, 0, 0, 0, 0, 0],\n","    90: [0, 0, 0, 0, 0, 0],\n","}\n","\n","df = pd.DataFrame(data)"],"metadata":{"id":"2XeznhTvB1VY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["format = Format()\n","format.scale = 0\n","format.cns = 1\n","format.ia = 1\n","format.imp = 0\n","# Process the dataset by applying Common Negative Scaling (CNS) and Independent Analysis (IA)\n","dataset_it, labels_it, format = process(dataset.copy(), labels.copy(), format)\n","# Flatten the labels for easier indexing\n","labels_it = labels_it.flatten()\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'Level for Diff'\n","dataset_temp = []\n","for line in dataset_it[labels_it == 1]:\n","  series = [y for y in line if y != -1]\n","  if len(series) == 0:\n","    continue\n","  dataset_temp.append(np.mean(series))\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"Level for Diff\", p] = float(np.percentile(dataset_temp, p))\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'Level for Undiff'\n","dataset_temp = []\n","for line in dataset_it[labels_it == 0]:\n","  series = [y for y in line if y != -1]\n","  if len(series) == 0:\n","    continue\n","  dataset_temp.append(np.mean(series))\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"Level for Undiff\", p] = np.percentile(dataset_temp, p)\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'Trend (SMD/Sesh)'\n","dataset_temp = []\n","for line in dataset_it:\n","  series = [y for y in line if y != -1]\n","  if len(series) == 0:\n","    continue\n","  # Perform linear regression to find the slope (trend)\n","  a, b = np.polyfit(range(len(series)), series, 1)\n","  dataset_temp.append(a)\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"Trend (SMD/Sesh)\", p] = np.percentile(dataset_temp, p)\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'Variability'\n","dataset_temp = []\n","for line in dataset_it:\n","  series = [y for y in line if y != -1]\n","  if len(series) == 0:\n","    continue\n","  # Perform linear regression to remove the trend\n","  a, b = np.polyfit(range(len(series)), series, 1)\n","  # Calculate the standard deviation of residuals (variability after trend removal)\n","  error = [y - (a*x + b) for x, y in enumerate(series)]\n","  dataset_temp.append(np.std(error))\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"Variability\", p] = np.percentile(dataset_temp, p)\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'CV' (Coefficient of Variation)\n","dataset_temp = []\n","for line in dataset_it:\n","  series = [y for y in line if y != -1]\n","  if len(series) == 0:\n","    continue\n","  std = np.std(series).clip(epsilon, None)\n","  # Calculate the absolute value of the mean divided by the standard deviation\n","  dataset_temp.append(abs(np.mean(series)) / std)\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"CV\", p] = np.percentile(dataset_temp, p)\n","\n","format = Format()\n","format.scale = 0\n","format.cns = 0\n","format.ia = 1\n","format.imp = 0\n","# Process the dataset again, this time only applying Individual Analysis (IA)\n","dataset_it, labels_it, format = process(dataset.copy(), labels.copy(), format)\n","labels_it = labels_it.flatten()\n","\n","# Calculate and store the 10th, 50th, and 90th percentiles for 'Trend (deg)'\n","dataset_temp = []\n","for line in dataset_it:\n","  # Consider the second series in each line (experimental series)\n","  series = [y for y in line[1] if y != -1]\n","  if len(series) == 0:\n","    continue\n","  # Perform linear regression to find the slope (trend)\n","  a, b = np.polyfit(range(len(series)), series, 1)\n","  # Convert the slope to degrees and append to temporary list\n","  dataset_temp.append(degrees(atan(a)))\n","for p in [10, 50, 90]:\n","  df.loc[df[\"Descriptor\"] == \"Trend (deg)\", p] = np.percentile(dataset_temp, p)"],"metadata":{"id":"RxaTWwEnDsGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df)"],"metadata":{"id":"6KaVJ8z_FQty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate Test Dataset"],"metadata":{"id":"hT_Tm5tnbSOb"}},{"cell_type":"code","source":["#Set random seed for replicability\n","np.random.seed(777)\n","\n","#Set values of autocorrelation 'a', trend in degrees 't', number of points in\n","#phase A and phase B, and standardized mean difference 'smd'\n","a_values = [0,0.2]\n","t_values = [0, 0, 0]\n","constant_values = [0.5]\n","points_values = (3,10)\n","smd_values = [3, 5, 7, 9, 10]\n","\n","series_dataset = []\n","series_labels = []\n","test_dataset = []\n","test_labels = []\n","# Generate individual time series based on parameter combinations\n","for i in range(10):\n","    for a in a_values:\n","        for t in t_values:\n","            for constant in constant_values:\n","                for points in range(points_values[0], points_values[1] + 1):\n","                      for smd in smd_values:\n","                          dataseries = create_series(a, t, constant,\n","                                                  points, smd)\n","                          series_dataset.append(dataseries)\n","                          series_labels.append(np.clip(smd, 0, 1))\n","\n","# Pair series with their labels and shuffle them randomly\n","paired = list(zip(series_dataset, series_labels))\n","random.shuffle(paired)\n","series_dataset, series_labels = zip(*paired)\n","\n","# Create 'graphs' by combining a control series with three generated series\n","for i in range(round(len(series_dataset)/3)):\n","  # Randomly determine autocorrelation for the control series\n","  a = a_values[-1] * random.randint(0, 1)\n","  constant = constant_values[0]\n","  points = random.randint(points_values[0], points_values[1])\n","  # Generate a control series with no SMD (effect)\n","  control = create_series(a, 0, constant, points, 0)\n","  # Assemble the graph: control + three experimental series\n","  graph = [control, series_dataset[i], series_dataset[i+1], series_dataset[i+2]]\n","  # Get labels for the experimental series\n","  graph_labels = [series_labels[i], series_labels[i+1], series_labels[i+2]]\n","  test_dataset.append(graph)\n","  test_labels.append(graph_labels)\n","\n","# Pad shorter series with -1 to a maximum length (max_dim)\n","for graph in range(len(test_dataset)):\n","  for line in range(len(test_dataset[graph])):\n","    length = len(test_dataset[graph][line])\n","    if length < 10:\n","      for i in range(10-length):\n","        test_dataset[graph][line].append(-1)\n","\n","# Convert the dataset and labels to NumPy arrays\n","test_dataset = np.array(test_dataset)\n","test_labels = np.array(test_labels)"],"metadata":{"id":"Cn0m6rmmbUNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'length of training dataset: {len(test_dataset)}')"],"metadata":{"id":"bXEwBqit_lkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('Datasets/test_dataset.npy', test_dataset)\n","np.save('Datasets/test_labels.npy', test_labels)"],"metadata":{"id":"CKIelP3yAaZc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate Effect Sizes"],"metadata":{"id":"O6fUu9PhzZEu"}},{"cell_type":"code","source":["def calc_effect_sizes(dataset):\n","\n","  real_effect_sizes = []\n","\n","  # Iterate through each graph in the dataset\n","  for graph in dataset:\n","    effect_sizes = []\n","    # Extract and filter the control series (first line in the graph)\n","    control = [i for i in graph[0] if i != -1]\n","    assert len(control) != 0\n","    # Calculate the mean and standard deviation of the control series\n","    c_mean = np.mean(control)\n","    c_std = np.std(control)\n","\n","    # Iterate through experimental lines (excluding the control line)\n","    for line in graph[1:]:\n","      # Extract and filter the current experimental line\n","      line = [i for i in line if i != -1]\n","      if len(line) == 0:\n","        # Assign -1 if the experimental line is empty\n","        effect_sizes.append(-1)\n","      elif c_std == 0:\n","        # Assign -2 if the control series has zero standard deviation (cannot normalize)\n","        effect_sizes.append(-2)\n","      else:\n","        # Calculate Cohen's d-like effect size for the current line\n","        es = np.absolute((np.mean(line) - c_mean) / c_std)\n","        effect_sizes.append(es)\n","    assert not np.any(np.isnan(effect_sizes))\n","    real_effect_sizes.append(effect_sizes)\n","\n","  return real_effect_sizes\n","\n","sim_effect_sizes = calc_effect_sizes(test_dataset)\n","np.save('Datasets/test_effect_sizes.npy', sim_effect_sizes)"],"metadata":{"id":"jy0laQXKzMht"},"execution_count":null,"outputs":[]}]}